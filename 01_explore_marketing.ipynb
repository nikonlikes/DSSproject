{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Multi-Target Marketing Campaign Forecasting Pipeline\n",
        "\n",
        "This notebook implements multi-target encoding and model training for:\n",
        "- **Conversion Rate** - Campaign conversion effectiveness\n",
        "- **Acquisition Cost** - Cost per customer acquisition\n",
        "- **Clicks** - Expected number of clicks\n",
        "- **Impressions** - Expected number of impressions\n",
        "- **Engagement Score** - Campaign engagement level\n",
        "\n",
        "The pipeline creates 5 separate models that can be used in the Streamlit dashboard for comprehensive campaign forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, validation_curve\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "print(\"Enhanced libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (200000, 16)\n",
            "Columns: ['Campaign_ID', 'Company', 'Campaign_Type', 'Target_Audience', 'Duration', 'Channel_Used', 'Conversion_Rate', 'Acquisition_Cost', 'ROI', 'Location', 'Language', 'Clicks', 'Impressions', 'Engagement_Score', 'Customer_Segment', 'Date']\n",
            "\n",
            "First few rows:\n",
            "   Campaign_ID              Company Campaign_Type Target_Audience Duration  \\\n",
            "0            1  Innovate Industries         Email       Men 18-24  30 days   \n",
            "1            2       NexGen Systems         Email     Women 35-44  60 days   \n",
            "2            3    Alpha Innovations    Influencer       Men 25-34  30 days   \n",
            "3            4   DataTech Solutions       Display        All Ages  60 days   \n",
            "4            5       NexGen Systems         Email       Men 25-34  15 days   \n",
            "\n",
            "  Channel_Used  Conversion_Rate Acquisition_Cost   ROI     Location  Language  \\\n",
            "0   Google Ads             0.04       $16,174.00  6.29      Chicago   Spanish   \n",
            "1   Google Ads             0.12       $11,566.00  5.61     New York    German   \n",
            "2      YouTube             0.07       $10,200.00  7.18  Los Angeles    French   \n",
            "3      YouTube             0.11       $12,724.00  5.55        Miami  Mandarin   \n",
            "4      YouTube             0.05       $16,452.00  6.50  Los Angeles  Mandarin   \n",
            "\n",
            "   Clicks  Impressions  Engagement_Score     Customer_Segment        Date  \n",
            "0     506         1922                 6    Health & Wellness  2021-01-01  \n",
            "1     116         7523                 7         Fashionistas  2021-01-02  \n",
            "2     584         7698                 1  Outdoor Adventurers  2021-01-03  \n",
            "3     217         1820                 7    Health & Wellness  2021-01-04  \n",
            "4     379         4201                 3    Health & Wellness  2021-01-05  \n"
          ]
        }
      ],
      "source": [
        "# Load raw data\n",
        "df = pd.read_csv('data/marketing_campaign_dataset.csv', low_memory=False)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working dataset shape after cleanup: (200000, 13)\n",
            "Columns remaining: ['Campaign_Type', 'Target_Audience', 'Duration', 'Channel_Used', 'Conversion_Rate', 'Acquisition_Cost', 'Location', 'Language', 'Clicks', 'Impressions', 'Engagement_Score', 'Customer_Segment', 'Date']\n",
            "\n",
            "Missing values:\n",
            "Campaign_Type       0\n",
            "Target_Audience     0\n",
            "Duration            0\n",
            "Channel_Used        0\n",
            "Conversion_Rate     0\n",
            "Acquisition_Cost    0\n",
            "Location            0\n",
            "Language            0\n",
            "Clicks              0\n",
            "Impressions         0\n",
            "Engagement_Score    0\n",
            "Customer_Segment    0\n",
            "Date                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Base cleanup - remove columns not needed for modeling\n",
        "working = df.drop(['ROI','Company','Campaign_ID'], axis=1)\n",
        "\n",
        "print(f\"Working dataset shape after cleanup: {working.shape}\")\n",
        "print(f\"Columns remaining: {working.columns.tolist()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(working.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acquisition_Cost data type: float64\n",
            "Acquisition_Cost range: $5000.00 - $20000.00\n",
            "Acquisition_Cost mean: $12504.39\n",
            "\n",
            "First few Acquisition_Cost values:\n",
            "0    16174.0\n",
            "1    11566.0\n",
            "2    10200.0\n",
            "3    12724.0\n",
            "4    16452.0\n",
            "Name: Acquisition_Cost, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Convert Acquisition_Cost to numeric (remove $ and commas)\n",
        "working['Acquisition_Cost'] = (working['Acquisition_Cost']\n",
        "                               .str.replace(r'[$,]', '', regex=True)\n",
        "                               .astype(float))\n",
        "\n",
        "print(f\"Acquisition_Cost data type: {working['Acquisition_Cost'].dtype}\")\n",
        "print(f\"Acquisition_Cost range: ${working['Acquisition_Cost'].min():.2f} - ${working['Acquisition_Cost'].max():.2f}\")\n",
        "print(f\"Acquisition_Cost mean: ${working['Acquisition_Cost'].mean():.2f}\")\n",
        "print(\"\\nFirst few Acquisition_Cost values:\")\n",
        "print(working['Acquisition_Cost'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Month values: ['April', 'August', 'December', 'February', 'January', 'July', 'June', 'March', 'May', 'November', 'October', 'September']\n",
            "Month value counts:\n",
            "January      16988\n",
            "March        16988\n",
            "May          16988\n",
            "July         16988\n",
            "August       16988\n",
            "October      16988\n",
            "December     16968\n",
            "April        16440\n",
            "June         16440\n",
            "September    16440\n",
            "November     16440\n",
            "February     15344\n",
            "Name: Month, dtype: int64\n",
            "\n",
            "Columns after Date processing:\n",
            "['Campaign_Type', 'Target_Audience', 'Duration', 'Channel_Used', 'Conversion_Rate', 'Acquisition_Cost', 'Location', 'Language', 'Clicks', 'Impressions', 'Engagement_Score', 'Customer_Segment', 'Month']\n",
            "\n",
            "Month distribution:\n",
            "  January: 16,988 campaigns\n",
            "  February: 15,344 campaigns\n",
            "  March: 16,988 campaigns\n",
            "  April: 16,440 campaigns\n",
            "  May: 16,988 campaigns\n",
            "  June: 16,440 campaigns\n",
            "  July: 16,988 campaigns\n",
            "  August: 16,988 campaigns\n",
            "  September: 16,440 campaigns\n",
            "  October: 16,988 campaigns\n",
            "  November: 16,440 campaigns\n",
            "  December: 16,968 campaigns\n"
          ]
        }
      ],
      "source": [
        "# Extract month names from Date column and drop original Date column\n",
        "working['Date_parsed'] = pd.to_datetime(working['Date'])\n",
        "working['Month'] = working['Date_parsed'].dt.month_name()\n",
        "working = working.drop(columns=['Date', 'Date_parsed'])\n",
        "\n",
        "print(f\"Month values: {sorted(working['Month'].unique())}\")\n",
        "print(f\"Month value counts:\")\n",
        "print(working['Month'].value_counts())\n",
        "print(\"\\nColumns after Date processing:\")\n",
        "print(working.columns.tolist())\n",
        "\n",
        "# Display month distribution\n",
        "print(f\"\\nMonth distribution:\")\n",
        "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
        "               'July', 'August', 'September', 'October', 'November', 'December']\n",
        "for month in month_order:\n",
        "    if month in working['Month'].values:\n",
        "        count = working['Month'].value_counts()[month]\n",
        "        print(f\"  {month}: {count:,} campaigns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duration values (original): ['15 days', '30 days', '45 days', '60 days']\n",
            "Duration value counts:\n",
            "30 days    50255\n",
            "45 days    50100\n",
            "60 days    49866\n",
            "15 days    49779\n",
            "Name: Duration, dtype: int64\n",
            "\n",
            "Duration data verification:\n",
            "  Total campaigns: 200,000\n",
            "  Unique durations: 4\n",
            "  Missing values: 0\n",
            "\n",
            "Duration distribution:\n",
            "  15 days: 49,779 campaigns (24.9%)\n",
            "  30 days: 50,255 campaigns (25.1%)\n",
            "  45 days: 50,100 campaigns (25.1%)\n",
            "  60 days: 49,866 campaigns (24.9%)\n"
          ]
        }
      ],
      "source": [
        "# Keep Duration as categorical for one-hot encoding (no numeric mapping)\n",
        "print(f\"Duration values (original): {sorted(working['Duration'].unique())}\")\n",
        "print(f\"Duration value counts:\")\n",
        "print(working['Duration'].value_counts())\n",
        "\n",
        "# Verify data integrity\n",
        "print(f\"\\nDuration data verification:\")\n",
        "print(f\"  Total campaigns: {len(working):,}\")\n",
        "print(f\"  Unique durations: {working['Duration'].nunique()}\")\n",
        "print(f\"  Missing values: {working['Duration'].isnull().sum()}\")\n",
        "\n",
        "# Display duration distribution\n",
        "print(f\"\\nDuration distribution:\")\n",
        "duration_order = ['15 days', '30 days', '45 days', '60 days']\n",
        "for duration in duration_order:\n",
        "    if duration in working['Duration'].values:\n",
        "        count = working['Duration'].value_counts()[duration]\n",
        "        percentage = count / len(working) * 100\n",
        "        print(f\"  {duration}: {count:,} campaigns ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical columns to encode:\n",
            "  Campaign_Type: 5 unique values -> ['Display', 'Email', 'Influencer', 'Search', 'Social Media']\n",
            "  Target_Audience: 5 unique values -> ['All Ages', 'Men 18-24', 'Men 25-34', 'Women 25-34', 'Women 35-44']\n",
            "  Channel_Used: 6 unique values -> ['Email', 'Facebook', 'Google Ads', 'Instagram', 'Website', 'YouTube']\n",
            "  Location: 5 unique values -> ['Chicago', 'Houston', 'Los Angeles', 'Miami', 'New York']\n",
            "  Language: 5 unique values -> ['English', 'French', 'German', 'Mandarin', 'Spanish']\n",
            "  Customer_Segment: 5 unique values -> ['Fashionistas', 'Foodies', 'Health & Wellness', 'Outdoor Adventurers', 'Tech Enthusiasts']\n",
            "  Month: 12 unique values -> ['April', 'August', 'December', 'February', 'January', 'July', 'June', 'March', 'May', 'November', 'October', 'September']\n",
            "  Duration: 4 unique values -> ['15 days', '30 days', '45 days', '60 days']\n",
            "\n",
            "Shape before encoding: (200000, 13)\n",
            "Shape after encoding: (200000, 52)\n",
            "New columns created: 39\n",
            "\n",
            "First 10 columns after encoding:\n",
            "['Conversion_Rate', 'Acquisition_Cost', 'Clicks', 'Impressions', 'Engagement_Score', 'Campaign_Type_Display', 'Campaign_Type_Email', 'Campaign_Type_Influencer', 'Campaign_Type_Search', 'Campaign_Type_Social Media']\n",
            "\n",
            "Month columns created:\n",
            "  ['Month_April', 'Month_August', 'Month_December', 'Month_February', 'Month_January', 'Month_July', 'Month_June', 'Month_March', 'Month_May', 'Month_November', 'Month_October', 'Month_September']\n",
            "\n",
            "Duration columns created:\n",
            "  ['Duration_15 days', 'Duration_30 days', 'Duration_45 days', 'Duration_60 days']\n",
            "\n",
            "Total feature categories:\n",
            "  - Numeric features: 5\n",
            "  - Categorical features: 47\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode categorical variables (including Month and Duration)\n",
        "categorical_columns = [\n",
        "    'Campaign_Type', 'Target_Audience', 'Channel_Used',\n",
        "    'Location', 'Language', 'Customer_Segment', 'Month', 'Duration'\n",
        "]\n",
        "\n",
        "print(\"Categorical columns to encode:\")\n",
        "for col in categorical_columns:\n",
        "    print(f\"  {col}: {len(working[col].unique())} unique values -> {sorted(working[col].unique())}\")\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encodedData2 = pd.get_dummies(\n",
        "    working,\n",
        "    columns=categorical_columns,\n",
        "    drop_first=False\n",
        ")\n",
        "\n",
        "print(f\"\\nShape before encoding: {working.shape}\")\n",
        "print(f\"Shape after encoding: {encodedData2.shape}\")\n",
        "print(f\"New columns created: {encodedData2.shape[1] - working.shape[1]}\")\n",
        "\n",
        "# Display first few columns to verify\n",
        "print(\"\\nFirst 10 columns after encoding:\")\n",
        "print(encodedData2.columns[:10].tolist())\n",
        "\n",
        "# Show the new month and duration columns\n",
        "print(\"\\nMonth columns created:\")\n",
        "month_cols = [col for col in encodedData2.columns if col.startswith('Month_')]\n",
        "print(f\"  {month_cols}\")\n",
        "\n",
        "print(\"\\nDuration columns created:\")\n",
        "duration_cols = [col for col in encodedData2.columns if col.startswith('Duration_')]\n",
        "print(f\"  {duration_cols}\")\n",
        "\n",
        "print(f\"\\nTotal feature categories:\")\n",
        "print(f\"  - Numeric features: {len([col for col in encodedData2.columns if not any(col.startswith(prefix) for prefix in ['Campaign_Type_', 'Target_Audience_', 'Channel_Used_', 'Location_', 'Language_', 'Customer_Segment_', 'Month_', 'Duration_'])])}\")\n",
        "print(f\"  - Categorical features: {encodedData2.shape[1] - len([col for col in encodedData2.columns if not any(col.startswith(prefix) for prefix in ['Campaign_Type_', 'Target_Audience_', 'Channel_Used_', 'Location_', 'Language_', 'Customer_Segment_', 'Month_', 'Duration_'])])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Target columns renamed to lowercase\n",
            "Column mapping: {'Conversion_Rate': 'conversion_rate', 'Acquisition_Cost': 'acquisition_cost', 'Clicks': 'clicks', 'Impressions': 'impressions', 'Engagement_Score': 'engagement_score'}\n",
            "\n",
            "Data types in encoded dataset:\n",
            "uint8      47\n",
            "int64       3\n",
            "float64     2\n",
            "dtype: int64\n",
            "\n",
            "Target variables summary:\n",
            "  conversion_rate: float64, range: 0.01 - 0.15\n",
            "  acquisition_cost: float64, range: 5000.00 - 20000.00\n",
            "  clicks: int64, range: 100.00 - 1000.00\n",
            "  impressions: int64, range: 1000.00 - 10000.00\n",
            "  engagement_score: int64, range: 1.00 - 10.00\n",
            "\n",
            "Encoded data exported to 'data/df_encoded_v2.csv'\n",
            "File size: 16.59 MB in memory\n"
          ]
        }
      ],
      "source": [
        "# Rename target columns to lowercase for consistency\n",
        "target_mapping = {\n",
        "    'Conversion_Rate': 'conversion_rate',\n",
        "    'Acquisition_Cost': 'acquisition_cost', \n",
        "    'Clicks': 'clicks',\n",
        "    'Impressions': 'impressions',\n",
        "    'Engagement_Score': 'engagement_score'\n",
        "}\n",
        "\n",
        "# Rename the columns\n",
        "encodedData2 = encodedData2.rename(columns=target_mapping)\n",
        "\n",
        "print(\"✅ Target columns renamed to lowercase\")\n",
        "print(f\"Column mapping: {target_mapping}\")\n",
        "\n",
        "# Verify data types and export encoded data\n",
        "print(\"\\nData types in encoded dataset:\")\n",
        "print(encodedData2.dtypes.value_counts())\n",
        "\n",
        "print(\"\\nTarget variables summary:\")\n",
        "targets = ['conversion_rate', 'acquisition_cost', 'clicks', 'impressions', 'engagement_score']\n",
        "for target in targets:\n",
        "    if target in encodedData2.columns:\n",
        "        print(f\"  {target}: {encodedData2[target].dtype}, range: {encodedData2[target].min():.2f} - {encodedData2[target].max():.2f}\")\n",
        "    else:\n",
        "        print(f\"  {target}: NOT FOUND in dataset\")\n",
        "\n",
        "# Export encoded data\n",
        "encodedData2.to_csv('data/df_encoded_v2.csv', index=False)\n",
        "print(f\"\\nEncoded data exported to 'data/df_encoded_v2.csv'\")\n",
        "print(f\"File size: {encodedData2.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB in memory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All target variables found in dataset\n",
            "\n",
            "Feature matrix shape: (200000, 47)\n",
            "Number of features: 47\n",
            "\n",
            "First 10 feature names:\n",
            "['Campaign_Type_Display', 'Campaign_Type_Email', 'Campaign_Type_Influencer', 'Campaign_Type_Search', 'Campaign_Type_Social Media', 'Target_Audience_All Ages', 'Target_Audience_Men 18-24', 'Target_Audience_Men 25-34', 'Target_Audience_Women 25-34', 'Target_Audience_Women 35-44']\n",
            "\n",
            "📊 Updated target variables:\n",
            "  ✅ conversion_rate\n",
            "  ✅ acquisition_cost\n",
            "  ✅ clicks\n",
            "  ✅ impressions\n",
            "  ✅ engagement_score\n"
          ]
        }
      ],
      "source": [
        "# Train/test split - we'll use the same split for all targets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define target variables (now using lowercase names)\n",
        "targets = ['conversion_rate', 'acquisition_cost', 'clicks', 'impressions', 'engagement_score']\n",
        "\n",
        "# Verify all targets exist in the dataset\n",
        "missing_targets = [t for t in targets if t not in encodedData2.columns]\n",
        "if missing_targets:\n",
        "    print(f\"Warning: Missing target variables: {missing_targets}\")\n",
        "    print(f\"Available columns: {encodedData2.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"✅ All target variables found in dataset\")\n",
        "\n",
        "# Create feature matrix (X) by dropping all target variables\n",
        "X = encodedData2.drop(targets, axis=1)\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "\n",
        "# Display feature names (first 10)\n",
        "print(\"\\nFirst 10 feature names:\")\n",
        "print(X.columns[:10].tolist())\n",
        "\n",
        "print(f\"\\n📊 Updated target variables:\")\n",
        "for target in targets:\n",
        "    print(f\"  ✅ {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced model training functions defined ✓\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Model Zoo and Training Functions\n",
        "def create_model_zoo():\n",
        "    \"\"\"\n",
        "    Create a comprehensive model zoo with multiple algorithms\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary of model configurations\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        'HistGradientBoosting': HistGradientBoostingRegressor(\n",
        "            max_iter=200,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            l2_regularization=1.0,\n",
        "            random_state=42\n",
        "        ),\n",
        "        'ElasticNet': ElasticNet(\n",
        "            alpha=0.1,\n",
        "            l1_ratio=0.5,\n",
        "            random_state=42,\n",
        "            max_iter=2000\n",
        "        ),\n",
        "        'Ridge': Ridge(\n",
        "            alpha=1.0,\n",
        "            random_state=42\n",
        "        ),\n",
        "        'Lasso': Lasso(\n",
        "            alpha=0.1,\n",
        "            random_state=42,\n",
        "            max_iter=2000\n",
        "        ),\n",
        "        'RandomForest': RandomForestRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    return models\n",
        "\n",
        "def evaluate_models_cv(models, X_train, y_train, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Evaluate multiple models using cross-validation\n",
        "    \n",
        "    Args:\n",
        "        models (dict): Dictionary of models to evaluate\n",
        "        X_train: Training features\n",
        "        y_train: Training target\n",
        "        cv_folds (int): Number of cross-validation folds\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Results dataframe\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"Evaluating {name}...\")\n",
        "        \n",
        "        # Cross-validation scores\n",
        "        mae_scores = -cross_val_score(model, X_train, y_train, \n",
        "                                     cv=cv_folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "        r2_scores = cross_val_score(model, X_train, y_train,\n",
        "                                   cv=cv_folds, scoring='r2', n_jobs=-1)\n",
        "        \n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'MAE_mean': mae_scores.mean(),\n",
        "            'MAE_std': mae_scores.std(),\n",
        "            'R2_mean': r2_scores.mean(),\n",
        "            'R2_std': r2_scores.std()\n",
        "        })\n",
        "        \n",
        "        print(f\"  MAE: {mae_scores.mean():.4f} ± {mae_scores.std():.4f}\")\n",
        "        print(f\"  R²:  {r2_scores.mean():.4f} ± {r2_scores.std():.4f}\")\n",
        "    \n",
        "    return pd.DataFrame(results).sort_values('R2_mean', ascending=False)\n",
        "\n",
        "def hyperparameter_tuning(model_name, model, X_train, y_train, param_grid, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning for a given model\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): Name of the model\n",
        "        model: The model instance\n",
        "        X_train: Training features\n",
        "        y_train: Training target\n",
        "        param_grid (dict): Parameter grid for tuning\n",
        "        cv_folds (int): Number of cross-validation folds\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (best_model, best_params, best_score)\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Hyperparameter Tuning: {model_name} ---\")\n",
        "    \n",
        "    # Use GridSearchCV for smaller grids, RandomizedSearchCV for larger ones\n",
        "    total_combinations = 1\n",
        "    for key, values in param_grid.items():\n",
        "        total_combinations *= len(values)\n",
        "    \n",
        "    if total_combinations <= 20:\n",
        "        search = GridSearchCV(\n",
        "            model, param_grid, cv=cv_folds, scoring='r2', \n",
        "            n_jobs=-1, verbose=1\n",
        "        )\n",
        "        search_type = \"GridSearchCV\"\n",
        "    else:\n",
        "        search = RandomizedSearchCV(\n",
        "            model, param_grid, n_iter=20, cv=cv_folds, scoring='r2',\n",
        "            n_jobs=-1, random_state=42, verbose=1\n",
        "        )\n",
        "        search_type = \"RandomizedSearchCV\"\n",
        "    \n",
        "    print(f\"Using {search_type} with {total_combinations} combinations\")\n",
        "    \n",
        "    # Fit the search\n",
        "    search.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {search.best_params_}\")\n",
        "    print(f\"Best R² score: {search.best_score_:.4f}\")\n",
        "    \n",
        "    return search.best_estimator_, search.best_params_, search.best_score_\n",
        "\n",
        "def train_optimized_model(y_name, models, X_train, X_test, y_train, y_test, tune_hyperparameters=True):\n",
        "    \"\"\"\n",
        "    Train an optimized model for a given target variable with enhanced one-hot encoding\n",
        "    \n",
        "    Args:\n",
        "        y_name (str): Name of the target variable\n",
        "        models (dict): Dictionary of models to evaluate\n",
        "        X_train, X_test, y_train, y_test: Train/test split data\n",
        "        tune_hyperparameters (bool): Whether to perform hyperparameter tuning\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (best_model, metrics)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"OPTIMIZED TRAINING FOR {y_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Step 1: Model comparison with cross-validation\n",
        "    print(\"\\n1. Model Comparison with Cross-Validation\")\n",
        "    cv_results = evaluate_models_cv(models, X_train, y_train, cv_folds=3)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\nCross-validation results:\")\n",
        "    print(cv_results)\n",
        "    \n",
        "    # Get best model\n",
        "    best_model_name = cv_results.iloc[0]['Model']\n",
        "    best_model = models[best_model_name]\n",
        "    \n",
        "    print(f\"\\nBest model: {best_model_name}\")\n",
        "    \n",
        "    # Step 2: Hyperparameter tuning (if enabled)\n",
        "    if tune_hyperparameters:\n",
        "        # Define parameter grids\n",
        "        param_grids = {\n",
        "            'HistGradientBoosting': {\n",
        "                'max_iter': [100, 200, 300],\n",
        "                'max_depth': [6, 8, 10],\n",
        "                'learning_rate': [0.05, 0.1, 0.15],\n",
        "                'l2_regularization': [0.5, 1.0, 2.0]\n",
        "            },\n",
        "            'ElasticNet': {\n",
        "                'alpha': [0.01, 0.1, 1.0, 10.0],\n",
        "                'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "            },\n",
        "            'Lasso': {\n",
        "                'alpha': [0.01, 0.1, 1.0, 10.0]\n",
        "            },\n",
        "            'RandomForest': {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'max_depth': [8, 10, 12],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if best_model_name in param_grids:\n",
        "            best_model, best_params, best_score = hyperparameter_tuning(\n",
        "                best_model_name, best_model, X_train, y_train, \n",
        "                param_grids[best_model_name], cv_folds=3\n",
        "            )\n",
        "        else:\n",
        "            print(f\"No hyperparameter grid defined for {best_model_name}\")\n",
        "            best_model.fit(X_train, y_train)\n",
        "    else:\n",
        "        best_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Step 3: Final evaluation\n",
        "    print(\"\\n2. Final Model Evaluation\")\n",
        "    train_preds = best_model.predict(X_train)\n",
        "    test_preds = best_model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_mae = mean_absolute_error(y_train, train_preds)\n",
        "    train_r2 = r2_score(y_train, train_preds)\n",
        "    test_mae = mean_absolute_error(y_test, test_preds)\n",
        "    test_r2 = r2_score(y_test, test_preds)\n",
        "    \n",
        "    metrics = {\n",
        "        'train_mae': train_mae,\n",
        "        'train_r2': train_r2,\n",
        "        'test_mae': test_mae,\n",
        "        'test_r2': test_r2,\n",
        "        'best_model_name': best_model_name\n",
        "    }\n",
        "    \n",
        "    print(f\"Training MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
        "    print(f\"Test MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
        "    \n",
        "    # Step 4: Save model\n",
        "    model_filename = f'models/{y_name.lower()}_model.pkl'\n",
        "    joblib.dump(best_model, model_filename, compress=3)\n",
        "    print(f\"Model saved to: {model_filename}\")\n",
        "    \n",
        "    return best_model, metrics\n",
        "\n",
        "print(\"Enhanced model training functions defined ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ENHANCED multi-target model training...\n",
            "================================================================================\n",
            "Feature matrix shape: (200000, 47)\n",
            "Target variables: ['conversion_rate', 'acquisition_cost', 'clicks', 'impressions', 'engagement_score']\n",
            "\n",
            "Model zoo contains: ['HistGradientBoosting', 'ElasticNet', 'Ridge', 'Lasso', 'RandomForest']\n",
            "\n",
            "\n",
            "🎯 TARGET: conversion_rate\n",
            "================================================================================\n",
            "Training set size: 160000\n",
            "Test set size: 40000\n",
            "Target range: 0.0100 - 0.1500\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED TRAINING FOR conversion_rate\n",
            "============================================================\n",
            "\n",
            "1. Model Comparison with Cross-Validation\n",
            "Evaluating HistGradientBoosting...\n",
            "  MAE: 0.0350 ± 0.0001\n",
            "  R²:  -0.0003 ± 0.0001\n",
            "Evaluating ElasticNet...\n",
            "  MAE: 0.0350 ± 0.0001\n",
            "  R²:  -0.0001 ± 0.0000\n",
            "Evaluating Ridge...\n",
            "  MAE: 0.0351 ± 0.0001\n",
            "  R²:  -0.0004 ± 0.0002\n",
            "Evaluating Lasso...\n",
            "  MAE: 0.0350 ± 0.0001\n",
            "  R²:  -0.0001 ± 0.0000\n",
            "Evaluating RandomForest...\n",
            "  MAE: 0.0351 ± 0.0001\n",
            "  R²:  -0.0016 ± 0.0001\n",
            "\n",
            "Cross-validation results:\n",
            "                  Model  MAE_mean   MAE_std   R2_mean    R2_std\n",
            "1            ElasticNet  0.035025  0.000098 -0.000061  0.000043\n",
            "3                 Lasso  0.035025  0.000098 -0.000061  0.000043\n",
            "0  HistGradientBoosting  0.035047  0.000095 -0.000308  0.000053\n",
            "2                 Ridge  0.035059  0.000096 -0.000448  0.000159\n",
            "4          RandomForest  0.035106  0.000092 -0.001630  0.000069\n",
            "\n",
            "Best model: ElasticNet\n",
            "\n",
            "--- Hyperparameter Tuning: ElasticNet ---\n",
            "Using GridSearchCV with 16 combinations\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters: {'alpha': 0.01, 'l1_ratio': 0.1}\n",
            "Best R² score: -0.0001\n",
            "\n",
            "2. Final Model Evaluation\n",
            "Training MAE: 0.0350, R²: 0.0000\n",
            "Test MAE: 0.0349, R²: -0.0000\n",
            "Model saved to: models/conversion_rate_model.pkl\n",
            "✅ conversion_rate training completed!\n",
            "\n",
            "\n",
            "🎯 TARGET: acquisition_cost\n",
            "================================================================================\n",
            "Training set size: 160000\n",
            "Test set size: 40000\n",
            "Target range: 5000.0000 - 20000.0000\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED TRAINING FOR acquisition_cost\n",
            "============================================================\n",
            "\n",
            "1. Model Comparison with Cross-Validation\n",
            "Evaluating HistGradientBoosting...\n",
            "  MAE: 3758.9398 ± 13.2880\n",
            "  R²:  -0.0004 ± 0.0002\n",
            "Evaluating ElasticNet...\n",
            "  MAE: 3758.7715 ± 13.4972\n",
            "  R²:  -0.0002 ± 0.0000\n",
            "Evaluating Ridge...\n",
            "  MAE: 3758.9391 ± 13.5009\n",
            "  R²:  -0.0003 ± 0.0000\n",
            "Evaluating Lasso...\n",
            "  MAE: 3758.9217 ± 13.5040\n",
            "  R²:  -0.0003 ± 0.0000\n",
            "Evaluating RandomForest...\n",
            "  MAE: 3760.6699 ± 13.1038\n",
            "  R²:  -0.0018 ± 0.0002\n",
            "\n",
            "Cross-validation results:\n",
            "                  Model     MAE_mean    MAE_std   R2_mean    R2_std\n",
            "1            ElasticNet  3758.771505  13.497188 -0.000200  0.000034\n",
            "3                 Lasso  3758.921658  13.503995 -0.000320  0.000013\n",
            "2                 Ridge  3758.939114  13.500915 -0.000332  0.000013\n",
            "0  HistGradientBoosting  3758.939832  13.287990 -0.000393  0.000203\n",
            "4          RandomForest  3760.669913  13.103790 -0.001790  0.000188\n",
            "\n",
            "Best model: ElasticNet\n",
            "\n",
            "--- Hyperparameter Tuning: ElasticNet ---\n",
            "Using GridSearchCV with 16 combinations\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters: {'alpha': 10.0, 'l1_ratio': 0.9}\n",
            "Best R² score: -0.0000\n",
            "\n",
            "2. Final Model Evaluation\n",
            "Training MAE: 3758.4876, R²: 0.0000\n",
            "Test MAE: 3753.7111, R²: -0.0000\n",
            "Model saved to: models/acquisition_cost_model.pkl\n",
            "✅ acquisition_cost training completed!\n",
            "\n",
            "\n",
            "🎯 TARGET: clicks\n",
            "================================================================================\n",
            "Training set size: 160000\n",
            "Test set size: 40000\n",
            "Target range: 100.0000 - 1000.0000\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED TRAINING FOR clicks\n",
            "============================================================\n",
            "\n",
            "1. Model Comparison with Cross-Validation\n",
            "Evaluating HistGradientBoosting...\n",
            "  MAE: 225.2492 ± 0.4039\n",
            "  R²:  -0.0003 ± 0.0002\n",
            "Evaluating ElasticNet...\n",
            "  MAE: 225.2400 ± 0.4223\n",
            "  R²:  -0.0001 ± 0.0000\n",
            "Evaluating Ridge...\n",
            "  MAE: 225.2551 ± 0.4219\n",
            "  R²:  -0.0003 ± 0.0001\n",
            "Evaluating Lasso...\n",
            "  MAE: 225.2431 ± 0.4227\n",
            "  R²:  -0.0002 ± 0.0001\n",
            "Evaluating RandomForest...\n",
            "  MAE: 225.3355 ± 0.3930\n",
            "  R²:  -0.0014 ± 0.0002\n",
            "\n",
            "Cross-validation results:\n",
            "                  Model    MAE_mean   MAE_std   R2_mean    R2_std\n",
            "1            ElasticNet  225.240017  0.422343 -0.000143  0.000039\n",
            "3                 Lasso  225.243143  0.422682 -0.000181  0.000054\n",
            "0  HistGradientBoosting  225.249191  0.403901 -0.000271  0.000155\n",
            "2                 Ridge  225.255138  0.421908 -0.000328  0.000072\n",
            "4          RandomForest  225.335485  0.393045 -0.001420  0.000228\n",
            "\n",
            "Best model: ElasticNet\n",
            "\n",
            "--- Hyperparameter Tuning: ElasticNet ---\n",
            "Using GridSearchCV with 16 combinations\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters: {'alpha': 1.0, 'l1_ratio': 0.9}\n",
            "Best R² score: -0.0000\n",
            "\n",
            "2. Final Model Evaluation\n",
            "Training MAE: 225.2292, R²: 0.0000\n",
            "Test MAE: 224.9940, R²: -0.0002\n",
            "Model saved to: models/clicks_model.pkl\n",
            "✅ clicks training completed!\n",
            "\n",
            "\n",
            "🎯 TARGET: impressions\n",
            "================================================================================\n",
            "Training set size: 160000\n",
            "Test set size: 40000\n",
            "Target range: 1000.0000 - 10000.0000\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED TRAINING FOR impressions\n",
            "============================================================\n",
            "\n",
            "1. Model Comparison with Cross-Validation\n",
            "Evaluating HistGradientBoosting...\n",
            "  MAE: 2246.9558 ± 5.7268\n",
            "  R²:  -0.0004 ± 0.0003\n",
            "Evaluating ElasticNet...\n",
            "  MAE: 2246.7191 ± 5.7192\n",
            "  R²:  -0.0001 ± 0.0000\n",
            "Evaluating Ridge...\n",
            "  MAE: 2246.8140 ± 5.7591\n",
            "  R²:  -0.0003 ± 0.0001\n",
            "Evaluating Lasso...\n",
            "  MAE: 2246.7988 ± 5.7555\n",
            "  R²:  -0.0003 ± 0.0001\n",
            "Evaluating RandomForest...\n",
            "  MAE: 2248.3288 ± 6.2731\n",
            "  R²:  -0.0020 ± 0.0004\n",
            "\n",
            "Cross-validation results:\n",
            "                  Model     MAE_mean   MAE_std   R2_mean    R2_std\n",
            "1            ElasticNet  2246.719140  5.719175 -0.000147  0.000040\n",
            "3                 Lasso  2246.798823  5.755509 -0.000258  0.000061\n",
            "2                 Ridge  2246.814046  5.759116 -0.000276  0.000064\n",
            "0  HistGradientBoosting  2246.955819  5.726752 -0.000408  0.000320\n",
            "4          RandomForest  2248.328776  6.273130 -0.001990  0.000450\n",
            "\n",
            "Best model: ElasticNet\n",
            "\n",
            "--- Hyperparameter Tuning: ElasticNet ---\n",
            "Using GridSearchCV with 16 combinations\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters: {'alpha': 1.0, 'l1_ratio': 0.5}\n",
            "Best R² score: -0.0000\n",
            "\n",
            "2. Final Model Evaluation\n",
            "Training MAE: 2246.5156, R²: 0.0001\n",
            "Test MAE: 2248.8538, R²: -0.0000\n",
            "Model saved to: models/impressions_model.pkl\n",
            "✅ impressions training completed!\n",
            "\n",
            "\n",
            "🎯 TARGET: engagement_score\n",
            "================================================================================\n",
            "Training set size: 160000\n",
            "Test set size: 40000\n",
            "Target range: 1.0000 - 10.0000\n",
            "\n",
            "============================================================\n",
            "OPTIMIZED TRAINING FOR engagement_score\n",
            "============================================================\n",
            "\n",
            "1. Model Comparison with Cross-Validation\n",
            "Evaluating HistGradientBoosting...\n",
            "  MAE: 2.4997 ± 0.0039\n",
            "  R²:  -0.0003 ± 0.0001\n",
            "Evaluating ElasticNet...\n",
            "  MAE: 2.4996 ± 0.0041\n",
            "  R²:  -0.0000 ± 0.0000\n",
            "Evaluating Ridge...\n",
            "  MAE: 2.4996 ± 0.0040\n",
            "  R²:  -0.0006 ± 0.0001\n",
            "Evaluating Lasso...\n",
            "  MAE: 2.4996 ± 0.0041\n",
            "  R²:  -0.0000 ± 0.0000\n",
            "Evaluating RandomForest...\n",
            "  MAE: 2.4994 ± 0.0037\n",
            "  R²:  -0.0014 ± 0.0003\n",
            "\n",
            "Cross-validation results:\n",
            "                  Model  MAE_mean   MAE_std   R2_mean    R2_std\n",
            "1            ElasticNet  2.499601  0.004064 -0.000022  0.000016\n",
            "3                 Lasso  2.499601  0.004064 -0.000022  0.000016\n",
            "0  HistGradientBoosting  2.499654  0.003880 -0.000325  0.000124\n",
            "2                 Ridge  2.499607  0.004011 -0.000582  0.000105\n",
            "4          RandomForest  2.499380  0.003743 -0.001400  0.000272\n",
            "\n",
            "Best model: ElasticNet\n",
            "\n",
            "--- Hyperparameter Tuning: ElasticNet ---\n",
            "Using GridSearchCV with 16 combinations\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters: {'alpha': 0.01, 'l1_ratio': 0.9}\n",
            "Best R² score: -0.0000\n",
            "\n",
            "2. Final Model Evaluation\n",
            "Training MAE: 2.4996, R²: 0.0000\n",
            "Test MAE: 2.5056, R²: -0.0001\n",
            "Model saved to: models/engagement_score_model.pkl\n",
            "✅ engagement_score training completed!\n",
            "\n",
            "================================================================================\n",
            "🎉 ALL TRAINING COMPLETE - COMPREHENSIVE RESULTS\n",
            "================================================================================\n",
            "\n",
            "Detailed Results:\n",
            "                    train_mae  train_r2     test_mae   test_r2 best_model_name\n",
            "conversion_rate      0.035022       0.0     0.034905 -0.000013      ElasticNet\n",
            "acquisition_cost  3758.487582  0.000004  3753.711103 -0.000004      ElasticNet\n",
            "clicks             225.229237       0.0   224.993962 -0.000178      ElasticNet\n",
            "impressions       2246.515567  0.000118  2248.853767  -0.00003      ElasticNet\n",
            "engagement_score     2.499588       0.0     2.505575 -0.000086      ElasticNet\n",
            "\n",
            "📊 SUMMARY TABLE:\n",
            "============================================================\n",
            "conversion_rate      | Test MAE: 0.0349   | Test R²: -0.0000  | Model: ElasticNet\n",
            "acquisition_cost     | Test MAE: 3753.7111 | Test R²: -0.0000  | Model: ElasticNet\n",
            "clicks               | Test MAE: 224.9940 | Test R²: -0.0002  | Model: ElasticNet\n",
            "impressions          | Test MAE: 2248.8538 | Test R²: -0.0000  | Model: ElasticNet\n",
            "engagement_score     | Test MAE: 2.5056   | Test R²: -0.0001  | Model: ElasticNet\n",
            "\n",
            "All optimized models saved to 'models/' directory\n",
            "Total models trained: 5\n",
            "\n",
            "🔍 PERFORMANCE INSIGHTS:\n",
            "========================================\n",
            "1. acquisition_cost: R² = -0.0000 (ElasticNet)\n",
            "2. conversion_rate: R² = -0.0000 (ElasticNet)\n",
            "3. impressions: R² = -0.0000 (ElasticNet)\n",
            "\n",
            "🏆 MODEL USAGE SUMMARY:\n",
            "==============================\n",
            "ElasticNet: 5 targets -> conversion_rate, acquisition_cost, clicks, impressions, engagement_score\n",
            "\n",
            "✨ Training pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Multi-Target Model Training with Optimization\n",
        "targets = ['conversion_rate', 'acquisition_cost', 'clicks', 'impressions', 'engagement_score']\n",
        "\n",
        "print(\"Starting ENHANCED multi-target model training...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create train/test split once for all targets\n",
        "X = encodedData2.drop(targets, axis=1)\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target variables: {targets}\")\n",
        "\n",
        "# Initialize model zoo\n",
        "models = create_model_zoo()\n",
        "print(f\"\\nModel zoo contains: {list(models.keys())}\")\n",
        "\n",
        "# Train optimized models for all targets\n",
        "all_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for target in targets:\n",
        "    print(f\"\\n\\n🎯 TARGET: {target}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get target variable\n",
        "    y = encodedData2[target]\n",
        "    \n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set size: {X_train.shape[0]}\")\n",
        "    print(f\"Test set size: {X_test.shape[0]}\")\n",
        "    print(f\"Target range: {y.min():.4f} - {y.max():.4f}\")\n",
        "    \n",
        "    # Train optimized model\n",
        "    best_model, metrics = train_optimized_model(\n",
        "        target, models, X_train, X_test, y_train, y_test, \n",
        "        tune_hyperparameters=True\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    all_results[target] = metrics\n",
        "    trained_models[target] = best_model\n",
        "    \n",
        "    print(f\"✅ {target} training completed!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"🎉 ALL TRAINING COMPLETE - COMPREHENSIVE RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create comprehensive results table\n",
        "results_df = pd.DataFrame(all_results).T\n",
        "print(\"\\nDetailed Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Summary table\n",
        "print(\"\\n📊 SUMMARY TABLE:\")\n",
        "print(\"=\" * 60)\n",
        "for target, scores in all_results.items():\n",
        "    print(f\"{target:<20} | Test MAE: {scores['test_mae']:<8.4f} | Test R²: {scores['test_r2']:<8.4f} | Model: {scores['best_model_name']}\")\n",
        "\n",
        "print(f\"\\nAll optimized models saved to 'models/' directory\")\n",
        "print(f\"Total models trained: {len(all_results)}\")\n",
        "\n",
        "# Performance insights\n",
        "print(\"\\n🔍 PERFORMANCE INSIGHTS:\")\n",
        "print(\"=\" * 40)\n",
        "best_performers = sorted(all_results.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
        "for i, (target, metrics) in enumerate(best_performers[:3]):\n",
        "    print(f\"{i+1}. {target}: R² = {metrics['test_r2']:.4f} ({metrics['best_model_name']})\")\n",
        "\n",
        "# Model usage summary\n",
        "model_usage = {}\n",
        "for target, metrics in all_results.items():\n",
        "    model_name = metrics['best_model_name']\n",
        "    if model_name not in model_usage:\n",
        "        model_usage[model_name] = []\n",
        "    model_usage[model_name].append(target)\n",
        "\n",
        "print(\"\\n🏆 MODEL USAGE SUMMARY:\")\n",
        "print(\"=\" * 30)\n",
        "for model_name, targets_used in model_usage.items():\n",
        "    print(f\"{model_name}: {len(targets_used)} targets -> {', '.join(targets_used)}\")\n",
        "\n",
        "print(\"\\n✨ Training pipeline completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature names saved for Streamlit application\n",
            "Total features: 47\n",
            "Feature names saved to: models/feature_names_v2.pkl\n",
            "\n",
            "First 10 feature names:\n",
            "  1. Campaign_Type_Display\n",
            "  2. Campaign_Type_Email\n",
            "  3. Campaign_Type_Influencer\n",
            "  4. Campaign_Type_Search\n",
            "  5. Campaign_Type_Social Media\n",
            "  6. Target_Audience_All Ages\n",
            "  7. Target_Audience_Men 18-24\n",
            "  8. Target_Audience_Men 25-34\n",
            "  9. Target_Audience_Women 25-34\n",
            "  10. Target_Audience_Women 35-44\n",
            "\n",
            "Base feature categories:\n",
            "Numeric features: ['Duration', 'Acquisition_Cost', 'Month']\n",
            "Categorical features (one-hot encoded): ['Campaign_Type', 'Target_Audience', 'Channel_Used', 'Location', 'Language', 'Customer_Segment']\n"
          ]
        }
      ],
      "source": [
        "# Save feature names for Streamlit application\n",
        "feature_names = encodedData2.drop(targets, axis=1).columns.tolist()\n",
        "\n",
        "# Save feature names to pickle file\n",
        "joblib.dump(feature_names, 'models/feature_names_v2.pkl')\n",
        "\n",
        "print(\"Feature names saved for Streamlit application\")\n",
        "print(f\"Total features: {len(feature_names)}\")\n",
        "print(f\"Feature names saved to: models/feature_names_v2.pkl\")\n",
        "\n",
        "# Display first 10 feature names as verification\n",
        "print(\"\\nFirst 10 feature names:\")\n",
        "for i, name in enumerate(feature_names[:10]):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "# Also save the raw feature names for reference\n",
        "print(\"\\nBase feature categories:\")\n",
        "base_features = ['Duration', 'Acquisition_Cost', 'Month']\n",
        "categorical_features = ['Campaign_Type', 'Target_Audience', 'Channel_Used', 'Location', 'Language', 'Customer_Segment']\n",
        "print(f\"Numeric features: {base_features}\")\n",
        "print(f\"Categorical features (one-hot encoded): {categorical_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🔬 ADVANCED MODEL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "1. VALIDATION CURVE ANALYSIS\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Conversion_Rate'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/schoolML/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/anaconda3/envs/schoolML/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/schoolML/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Conversion_Rate'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[111], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Get a sample target for validation curve analysis\u001b[39;00m\n\u001b[1;32m     55\u001b[0m sample_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConversion_Rate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 56\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m encodedData2[sample_target]\n\u001b[1;32m     57\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m encodedData2\u001b[38;5;241m.\u001b[39mdrop(targets, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Create smaller sample for faster computation\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/schoolML/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/anaconda3/envs/schoolML/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Conversion_Rate'"
          ]
        }
      ],
      "source": [
        "# Advanced Model Analysis and Validation Curves\n",
        "print(\"=\" * 80)\n",
        "print(\"🔬 ADVANCED MODEL ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def plot_validation_curves(model_name, model, X_train, y_train, param_name, param_range):\n",
        "    \"\"\"\n",
        "    Plot validation curves for a specific parameter\n",
        "    \"\"\"\n",
        "    print(f\"\\nGenerating validation curve for {model_name} - {param_name}\")\n",
        "    \n",
        "    # Calculate validation curve\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        model, X_train, y_train, param_name=param_name, param_range=param_range,\n",
        "        cv=3, scoring='r2', n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Calculate mean and std\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(param_range, train_scores_mean, 'o-', color='blue', label='Training score')\n",
        "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
        "    \n",
        "    plt.plot(param_range, test_scores_mean, 'o-', color='red', label='Cross-validation score')\n",
        "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color='red')\n",
        "    \n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel('R² Score')\n",
        "    plt.title(f'Validation Curve - {model_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    # Find best parameter\n",
        "    best_idx = np.argmax(test_scores_mean)\n",
        "    best_param = param_range[best_idx]\n",
        "    best_score = test_scores_mean[best_idx]\n",
        "    \n",
        "    print(f\"Best {param_name}: {best_param} (R² = {best_score:.4f})\")\n",
        "    \n",
        "    return best_param, best_score\n",
        "\n",
        "# Analyze top performing models with validation curves\n",
        "print(\"\\n1. VALIDATION CURVE ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Get a sample target for validation curve analysis\n",
        "sample_target = 'conversion_rate'\n",
        "y_sample = encodedData2[sample_target]\n",
        "X_sample = encodedData2.drop(targets, axis=1)\n",
        "\n",
        "# Create smaller sample for faster computation\n",
        "sample_size = 5000\n",
        "sample_idx = np.random.choice(len(X_sample), min(sample_size, len(X_sample)), replace=False)\n",
        "X_sample_small = X_sample.iloc[sample_idx]\n",
        "y_sample_small = y_sample.iloc[sample_idx]\n",
        "\n",
        "print(f\"Using {len(X_sample_small)} samples for validation curve analysis\")\n",
        "\n",
        "# ElasticNet validation curve\n",
        "try:\n",
        "    elastic_net = ElasticNet(random_state=42, max_iter=2000)\n",
        "    alpha_range = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "    best_alpha, best_score = plot_validation_curves(\n",
        "        'ElasticNet', elastic_net, X_sample_small, y_sample_small, \n",
        "        'alpha', alpha_range\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"ElasticNet validation curve error: {e}\")\n",
        "\n",
        "# HistGradientBoosting validation curve\n",
        "try:\n",
        "    hist_gb = HistGradientBoostingRegressor(random_state=42)\n",
        "    learning_rate_range = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
        "    best_lr, best_score = plot_validation_curves(\n",
        "        'HistGradientBoosting', hist_gb, X_sample_small, y_sample_small,\n",
        "        'learning_rate', learning_rate_range\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"HistGradientBoosting validation curve error: {e}\")\n",
        "\n",
        "# 2. Model Comparison Visualization\n",
        "print(\"\\n2. MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Extract performance metrics for visualization\n",
        "target_names = list(all_results.keys())\n",
        "test_r2_scores = [all_results[target]['test_r2'] for target in target_names]\n",
        "test_mae_scores = [all_results[target]['test_mae'] for target in target_names]\n",
        "model_names = [all_results[target]['best_model_name'] for target in target_names]\n",
        "\n",
        "# Create comparison plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# R² scores\n",
        "bars1 = ax1.bar(range(len(target_names)), test_r2_scores, color='skyblue', alpha=0.7)\n",
        "ax1.set_xlabel('Target Variables')\n",
        "ax1.set_ylabel('R² Score')\n",
        "ax1.set_title('R² Scores by Target Variable')\n",
        "ax1.set_xticks(range(len(target_names)))\n",
        "ax1.set_xticklabels(target_names, rotation=45, ha='right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars1, test_r2_scores):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,\n",
        "             f'{score:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# MAE scores\n",
        "bars2 = ax2.bar(range(len(target_names)), test_mae_scores, color='lightcoral', alpha=0.7)\n",
        "ax2.set_xlabel('Target Variables')\n",
        "ax2.set_ylabel('Mean Absolute Error')\n",
        "ax2.set_title('MAE Scores by Target Variable')\n",
        "ax2.set_xticks(range(len(target_names)))\n",
        "ax2.set_xticklabels(target_names, rotation=45, ha='right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, score in zip(bars2, test_mae_scores):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(test_mae_scores)*0.01,\n",
        "             f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Model Algorithm Usage Analysis\n",
        "print(\"\\n3. MODEL ALGORITHM ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Count algorithm usage\n",
        "algorithm_counts = {}\n",
        "for target, metrics in all_results.items():\n",
        "    model_name = metrics['best_model_name']\n",
        "    if model_name not in algorithm_counts:\n",
        "        algorithm_counts[model_name] = 0\n",
        "    algorithm_counts[model_name] += 1\n",
        "\n",
        "# Create pie chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.pie(algorithm_counts.values(), labels=algorithm_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Best Model Distribution Across Targets')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "print(\"Algorithm usage:\")\n",
        "for model, count in algorithm_counts.items():\n",
        "    print(f\"  {model}: {count} targets ({count/len(all_results)*100:.1f}%)\")\n",
        "\n",
        "# 4. Feature Importance Analysis (for tree-based models)\n",
        "print(\"\\n4. FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X_sample.columns.tolist()\n",
        "\n",
        "# Analyze feature importance for tree-based models\n",
        "for target, model in trained_models.items():\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        print(f\"\\n{target} - Feature Importance (Top 10):\")\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(feature_importance.head(10))\n",
        "        \n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        top_features = feature_importance.head(15)\n",
        "        plt.barh(range(len(top_features)), top_features['importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title(f'Top 15 Features - {target}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        break  # Only show for first tree-based model\n",
        "\n",
        "print(\"\\n✅ Advanced model analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Enhanced Multi-Target Forecasting Pipeline Complete! 🎉\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook successfully created an **advanced multi-target forecasting pipeline** with state-of-the-art optimization techniques:\n",
        "\n",
        "### 🤖 **Advanced Model Zoo**\n",
        "- **HistGradientBoostingRegressor** - Gradient boosting with native optimization\n",
        "- **ElasticNet** - Regularized linear regression with L1/L2 penalties\n",
        "- **Ridge Regression** - L2 regularized linear model\n",
        "- **Lasso Regression** - L1 regularized linear model with feature selection\n",
        "- **Random Forest** - Ensemble of decision trees with bootstrap aggregation\n",
        "\n",
        "### 🎯 **Multi-Target Models**\n",
        "- **Conversion Rate Model** - Campaign conversion effectiveness prediction\n",
        "- **Acquisition Cost Model** - Cost per customer acquisition forecasting\n",
        "- **Clicks Model** - Expected click volume prediction\n",
        "- **Impressions Model** - Expected impression volume forecasting\n",
        "- **Engagement Score Model** - Campaign engagement level prediction\n",
        "\n",
        "### 📁 **Enhanced Files Created**\n",
        "- `models/conversion_rate_model.pkl` - Optimized model with best hyperparameters\n",
        "- `models/acquisition_cost_model.pkl` - Tuned model for cost prediction\n",
        "- `models/clicks_model.pkl` - Optimized click prediction model\n",
        "- `models/impressions_model.pkl` - Enhanced impression forecasting model\n",
        "- `models/engagement_score_model.pkl` - Tuned engagement prediction model\n",
        "- `models/feature_names_v2.pkl` - Feature metadata for Streamlit integration\n",
        "- `data/df_encoded_v2.csv` - Processed dataset with one-hot encoding\n",
        "\n",
        "### 🔧 **Enhanced Data Pipeline**\n",
        "1. ✅ Raw data loaded and cleaned (200K samples)\n",
        "2. ✅ Acquisition cost converted to numeric format\n",
        "3. ✅ Date features engineered (Month extraction)\n",
        "4. ✅ Duration mapped to numeric scale (1-4)\n",
        "5. ✅ Categorical variables one-hot encoded (31 dummy variables)\n",
        "6. ✅ **Multi-algorithm model comparison** across 5 different algorithms\n",
        "7. ✅ **Automated hyperparameter tuning** using GridSearchCV/RandomizedSearchCV\n",
        "8. ✅ **Cross-validation** for robust model evaluation\n",
        "9. ✅ **Validation curves** for parameter optimization analysis\n",
        "\n",
        "### 📊 **Advanced Optimization Techniques**\n",
        "- **Cross-Validation**: 3-fold CV for reliable performance estimation\n",
        "- **Hyperparameter Tuning**: GridSearchCV for small grids, RandomizedSearchCV for large spaces\n",
        "- **Model Comparison**: Automated best model selection per target\n",
        "- **Validation Curves**: Parameter sensitivity analysis\n",
        "- **Feature Importance**: Tree-based model interpretability\n",
        "- **Performance Visualization**: Comprehensive results analysis\n",
        "\n",
        "### 📈 **Performance Metrics & Analysis**\n",
        "Each model was evaluated using:\n",
        "- **MAE** (Mean Absolute Error) - Lower is better\n",
        "- **R²** (Coefficient of Determination) - Higher is better (max 1.0)\n",
        "- **Cross-validation scores** - Robust performance estimation\n",
        "- **Training vs. Test performance** - Overfitting detection\n",
        "- **Algorithm usage distribution** - Model selection insights\n",
        "- **Feature importance rankings** - Interpretability analysis\n",
        "\n",
        "### 🏆 **Key Improvements Over Basic Pipeline**\n",
        "1. **Multi-Algorithm Approach**: 5 different model types tested per target\n",
        "2. **Automated Hyperparameter Optimization**: GridSearchCV/RandomizedSearchCV\n",
        "3. **Cross-Validation**: Robust performance estimation\n",
        "4. **ElasticNet Integration**: Regularized linear models for better generalization\n",
        "5. **Validation Curves**: Parameter sensitivity analysis\n",
        "6. **Feature Importance**: Model interpretability\n",
        "7. **Comprehensive Visualization**: Performance analysis and insights\n",
        "8. **Memory Optimization**: Efficient processing for large datasets\n",
        "\n",
        "### 🚀 **Production Ready Features**\n",
        "1. **Streamlit Integration**: Compatible with existing dashboard\n",
        "2. **Model Persistence**: Optimized models saved with joblib compression\n",
        "3. **Feature Consistency**: Proper feature name mapping for predictions\n",
        "4. **Error Handling**: Robust error handling in training pipeline\n",
        "5. **Scalable Architecture**: Modular design for easy extension\n",
        "\n",
        "### 🎯 **Business Impact**\n",
        "- **Improved Accuracy**: Advanced algorithms and hyperparameter tuning\n",
        "- **Model Interpretability**: Feature importance analysis for business insights\n",
        "- **Robust Predictions**: Cross-validation ensures reliable performance\n",
        "- **Automated Optimization**: Reduces manual tuning effort\n",
        "- **Multi-Target Approach**: Comprehensive campaign analysis\n",
        "\n",
        "### 🔬 **Technical Achievements**\n",
        "- **Algorithm Diversity**: 5 different ML approaches tested\n",
        "- **Hyperparameter Optimization**: Automated parameter tuning\n",
        "- **Validation Analysis**: Learning curves and parameter sensitivity\n",
        "- **Feature Engineering**: One-hot encoding with 33 total features\n",
        "- **Performance Monitoring**: Train/test split analysis\n",
        "- **Memory Efficiency**: Optimized for 8GB RAM systems\n",
        "\n",
        "### 📋 **Next Steps**\n",
        "1. Run the Streamlit application: `streamlit run marketing_dss/app.py`\n",
        "2. Test multi-target predictions with optimized models\n",
        "3. Analyze model performance across different campaign scenarios\n",
        "4. Monitor feature importance for business insights\n",
        "5. Consider ensemble methods for further improvements\n",
        "\n",
        "The **enhanced pipeline** is now ready for production use with state-of-the-art optimization techniques! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "schoolML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

# Reuse processed dataset if available
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Define target and features (replace 'Conversion_Rate' if needed)
target = "Conversion_Rate"
X = df.drop(columns=[target])
y = df[target]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Compare model performance
results = []
for name, mdl in model_zoo.items():
    mdl.fit(X_train, y_train)
    preds = mdl.predict(X_test)
    r2 = r2_score(y_test, preds)
    results.append({"Model": name, "R2 Score": r2})

# Create DataFrame
results_df = pd.DataFrame(results).sort_values(by="R2 Score", ascending=False)

# Bar chart
plt.figure(figsize=(10, 5))
sns.barplot(x="R2 Score", y="Model", data=results_df, palette="magma")
plt.title("Model Comparison - R² Scores")
plt.xlabel("R² Score")
plt.ylabel("Model")
plt.xlim(0, 1)
plt.tight_layout()
plt.show()

# Display table
import matplotlib.pyplot as plt
import pandas as pd
from pandas.plotting import table

fig, ax = plt.subplots(figsize=(8, 2))
ax.axis("off")
tbl = table(ax, results_df.round(4), loc="center", cellLoc="center")
tbl.auto_set_font_size(False)
tbl.set_fontsize(10)
tbl.scale(1.2, 1.2)
plt.title("Model Performance Table")
plt.tight_layout()
plt.show()

print("📊 Model comparison completed!")

